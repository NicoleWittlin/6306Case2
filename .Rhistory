library(ggplot2)
data(wage)
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=.7, list = FALSE)
training <- wage[inTrain,]
training <- Wage[inTrain,]
test <- Wage[-inTrain,]
dim(training)
dim(training); dim(testing)
featurePlot(x=training[,c("age", "education", "jobclass")]), y = training$wage, plot = "pairs")
featurePlot(x=training[,c("age", "education", "jobclass")], y = training$wage, plot = "pairs")
qplot(age, wage, colour=jobclass, data = training)
qq <- qplot(age, wage, colour= education, data = training)
qq + geom_smooth(method='lm', formula = y~x)
install.packages("Hmisc")
install.packages("Hmisc")
knitr::opts_chunk$set(echo = TRUE)
library(Hmisc)
cutWage <- cut2(training$wage, g3)
cutWage <- cut2(training$wage, g=3)
table(cutWage)
p1 <- qplot(cutWage, age, data = training, fill = cutWage, geom=c("boxplot"))
p1
p1 <- qplot(cutWage, age, data = training, fill = cutWage, geom=c("boxplot", "jitter"))
p1
t1 <- table(cutWage, training$jobclass)
t1
prop.table(t1, 1)
prop.table(t1, 1)
qplot(wage, colour=education, data = training, geom="density")
hist(training$capitalAve, main = "", xlab="ave. capital run length")
hist(training$capitalAve)
set.seed(1235)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=.75, list = FALSE)
library(caret)   #
library(kernlab) #contains the data for spam data set
set.seed(1235)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=.75, list = FALSE)
#partitioning the data from spam that is in training and not in training
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve)
mean(training$capitalAve)
sd(training$capitalAve)
trainCapAveS < (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
mean(trainCapAveS)
sd(trainCapAveS)
testCapAveS <- (testCapAve - mean(trainCapAve))/sd(trainCapAve)
library(caret)
data("faithful")
data(faithful)
set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting, p = .5, list = FALSE)
trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue")
lm1 = lm(eruptions ~ waiting, data = trainFaith)
summary(lm1)
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue")
lines(trainFaith$waiting, lm1$fitted, lwd = 3)
coef(lm1[1] + coef(lm1)[2]*80)
coef(lm1)[1] + coef(lm1)[2]*80)
coef(lm1)[1] + coef(lm1)[2]*80
newdata <- data.frame(waiting = 80)
predict(lm1, newdata)
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
sqrt(sum((predict(lm1, newdata = testFaith)-testFaith$eruptions)^2))
pred1 <- predict(lm1, newdata= testFaith, interval= "prediction")
ord <- order(testFaith$waiting)
plot(testFaith, testFaith$eruptions, pch= 19, col = "blue")
matlines(testFaith$waiting[ord, pred1[ord,], type="l"],,col=c(1,2,2), lty = c(1,1,1), lwd=3)
matlines(testFaith$waiting[ord], pred1[ord,], type="l"],,col=c(1,2,2), lty = c(1,1,1), lwd=3)
matlines(testFaith$waiting[ord], pred1[ord,], type="l",,col=c(1,2,2), lty = c(1,1,1), lwd=3)
matlines(testFaith$waiting[ord], pred1[ord,], type="l",,col=c(1,2,2), lty = c(1,1,1), lwd=3)
modFit <- train(eruptions ~ waiting, data = trainFaith, method = "lm")
summary(modFit$finalModel)
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
Wage <- subset(Wage, select = -c(logwage))
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=.7, list = FALSE)
inTrain <- createDataPartition(y=Wage$wage, p=.7, list = FALSE)
training <- Wage[inTrain,]
test <- Wage[-inTrain,]
dim(training)
dim(test)
featurePlot(x=training[,c("age", "education", "jobclass")], y = training$wage, plot= "pairs")
qplot(age, wage, data = training)
qplot(age, wage, colour = jobclass, data = training)
qplot(age, wage, colour = education, data = training)
modFit <- train(wage ~ age + jobclass + education, method = "lm", data = training)
finMod <- modFit$finalModel
print(modFit)
modFit <- train(wage ~ age + jobclass + education, method = "lm", data = training)
finMod <- modFit$finalModel
print(modFit)
plot(finMod, 1, pch= 19, cex = .5, col="#000000010")
plot(finMod, 1, pch= 19, cex = .5, col="#00000010")
qplot(finMod$fitted, finMod$residuals, colour = race, data = training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(rattle)
install.packages("rattle")
data(iris)
data(iris)
library(ggplot2)
names(iris)
table(iris$Species)
data(iris)
library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p= .7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[inTrain,]
dim(training); dim(testing)
qplot(Petal.Width, Sepal.Width, colour = Species, data = training)
library(caret)
modFit <- train(Species ~., method = "rpart", data = training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFi
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
modFit <- train(Species ~., method = "rpart", data = training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex=.8)
install.packages("rattle")
library(rattle)
library(rattle)
library(rattle)
library(rattle)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
library(rattle)
library(RGtk2)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
installed.packages(rattle)
library(rattle)
install.packages(rattle)
install.packages("rattle")
library(rattle)
data(iris)
library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p= .7, list = FALSE)
data(iris)
library(ggplot2)
library(caret)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p= .7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[inTrain,]
dim(training); dim(testing)
qplot(Petal.Width, Sepal.Width, colour = Species, data = training)
modFit <- train(Species ~., method = "rpart", data = training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
mem.limits(size = 8000)
mem.limits(size = 6000)
memory.limit(size = 8000)
memory.limit(size = 80000)
#Load Libraries
library(caret)
library(randomForest)
#Set Working Direcotry
setwd("C:/Users/Marin Family/Documents/R/Machine Learning Assignment")
#Set Seed
set.seed(1234)
#Load Downloaded Train and Test Files from Working Directory
training <- read.csv(file="pml-training.csv", na.strings=c("", "NA", "NULL"))
testing <- read.csv(file="pml-testing.csv", na.strings=c("", "NA", "NULL"))
#Remove NA columns and indentifier columns
training <- training[, colSums(is.na(training))==0]
dim(training)
#After review, the first 7 columns as they do not seem relevant to the outcome.
training <- training[,-c(1:7)]
testing <- testing[, -c(1:7)]
dim(training)
dim(testing)
#Create my test set
inTrain <- createDataPartition(y=training$classe, p=.7, list = FALSE)
trainset <- training[inTrain,]
#Create My Test Data
mytestset <- training[-inTrain,]
dim(trainset)
dim(mytestset)
#Load Coursera Test Data
testset <- read.csv("pml-testing.csv")
dim(testset)
modelfit <- train(classe ~., data = trainset, method = "rf", prox = TRUE)
modelfit$finalmodel
modelfit$finalmodel
modelfit <- train(classe ~., data = trainset, method = "rf")
modelfit$finalmodel
fitControl <- trainControl(method = "cv",
number = 10,
allowParallel = TRUE)
modelfit <- train(classe ~., data = trainset, method = "rf", trControl = fitControl)
library(caret)
library(randomForest)
library(parallel)
library(doParallel)
install.packages("doParallel")
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
#Load Libraries
library(caret)
library(randomForest)
library(parallel)
library(doParallel)
#Set Working Direcotry
setwd("C:/Users/Marin Family/Documents/R/Machine Learning Assignment")
#Set Seed
set.seed(1234)
#Load Downloaded Train and Test Files from Working Directory
training <- read.csv(file="pml-training.csv", na.strings=c("", "NA", "NULL"))
testing <- read.csv(file="pml-testing.csv", na.strings=c("", "NA", "NULL"))
#Remove NA columns and indentifier columns
training <- training[, colSums(is.na(training))==0]
dim(training)
#After review, the first 7 columns as they do not seem relevant to the outcome.
training <- training[,-c(1:7)]
testing <- testing[, -c(1:7)]
dim(training)
dim(testing)
#Create my test set
inTrain <- createDataPartition(y=training$classe, p=.7, list = FALSE)
trainset <- training[inTrain,]
#Create My Test Data
mytestset <- training[-inTrain,]
dim(trainset)
dim(mytestset)
#Load Coursera Test Data
testset <- read.csv("pml-testing.csv")
dim(testset)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv",
number = 10,
allowParallel = TRUE)
modelfit <- train(classe ~., data = trainset, method = "rf", trControl = fitControl)
modelfit$finalmodel
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv",
number = 3,
allowParallel = TRUE)
modelfit <- train(classe ~., data = trainset, method = "rf", trControl = fitControl)
modelfit$finalmodel
corpus <- Corpus(VectorSource(c(blogs_sub, news_sub, twitter_sub)), readerControl=list(reader=readPlain,language="en"))
library(tm)
library(RWeka)
install.packages("ggplot")
corpus <- Corpus(VectorSource(c(blogs_sub, news_sub, twitter_sub)), readerControl=list(reader=readPlain,language="en"))
blogs <- readLines("./en_us.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
library(tm)
library(RWeka)
library(ggplot2)
library(tm)
library(rJava)
library(RWeka)
library(dplyr)
library(ggplot2)
setwd("C:/Users/Marin Family/Desktop/New folder/Coursera-SwiftKey/final/en_US")
getwdd()
getwd()
setwd("C:/Users/Marin Family/Desktop/New folder/Coursera-SwiftKey/final/en_US")
unlink('Capstone Week 2 Assignment_cache', recursive = TRUE)
setwd("C:/Users/Marin Family/Desktop/New folder/Coursera-SwiftKey/final/en_US")
library(tm)
library(rJava)
library(RWeka)
library(dplyr)
library(ggplot2)
setwd("C:/Users/Marin Family/Desktop/New folder/Coursera-SwiftKey/final/en_US")
?bonferroni
??bonferroni
install.packages("decstools")
install.packages("desctools")
install.packages("DescTools")
install.packages("xts")
install.packages("xts", repos="http://cloud.r-project.org")
?install.packages
install.packages("xts")
library(xts)
install.packages("xts")
installed.packages()
install.packages("xts")
install.packages("xts")
files
&&files
library("dygraphs")
library("xts")
library("forecast")
library(xlsx)
library(xts)
library(xts)
install.packages("xts")
library(xts)
install.packages("zoo")
library(xts)
?xts
colnames(Greg)[colnames(Wand)=="V1"] <- "Date"
library(dygraphs)
library(sqldf)
library(xts)
Greg <- read.csv("Unit11TimeSeries_Gregorovitch.csv", header = FALSE)
Ollie <- read.csv("Unit11TimeSeries_Ollivander.csv", header = FALSE)
class(Greg$V1)
Greg$V1 <- as.Date(Greg$V1, format = "%m/%d/%Y")
Ollie$V1 <- as.Date(Ollie$V1, format = "%m/%d/%Y")
Greg$Maker <- "Gregorovitch"
Ollie$Maker <- "Ollivander"
colnames(Greg)[colnames(Wand)=="V1"] <- "Date"
colnames(Greg)[colnames(Greg)=="V1"] <- "Date"
colnames(Greg)[colnames(Greg)=="V2"] <- "Units"
View(Greg)
colnames(Ollie)[colnames(Ollie)=="V1"] <- "Date"
colnames(Ollie)[colnames(Ollie)=="V2"] <- "Units"
xts(Greg$Units)
xts(Greg$Units, order.by = Greg$Date)
Olliexts <- xts(Ollie$Units, order.by = Ollie$Date)
Gregxts <- xts(Greg$Units, order.by = Greg$Date)
Wand <- merge(Gregxts, Olliexts)
?dySeries
#Live Session 12 demo
#install.packages("dygraphs")
library("dygraphs")
#install.packages("forecast")
library("forecast")
#read, convert and display births timeseries dataset
births <- scan("http://robjhyndman.com/tsdldata/data/nybirths.dat")
births <- ts(births, frequency = 12, start = c(1946, 1))  #frequency =12 because its a monthly dataset, 1=yearly, 4=quarterly
births
#read, convert and display gift shop dataset
gift <- scan("http://robjhyndman.com/tsdldata/data/fancy.dat")
gift<- ts(gift, frequency=12, start=c(1987,1))
gift
#plot births and gift timeseries - both have seasonal components
plot(births)  #additive or multiplicative?
plot(gift)    #additive or multiplicative?
abline(v=1992, col="red")
#decompose seasonal data
birthsComp <- decompose(births, type="add")
plot(birthsComp)
giftComp <- decompose(gift, type="mult")
plot(giftComp)
#HoltWinters filtering on births
birthsHW <- HoltWinters(births)
birthsHW
plot(birthsHW)
#forecast for next 48 months
#start from 1952
#blue line: actual prediction, dark gray: 80% confidence interval, light gray: 95% confidence interval
birthsSubset<-window(births, start=1952)
birthsF5 <- hw(birthsSubset, initial="optimal", h=48)
plot(birthsF5)
lines(fitted(birthsF5), col="red")
#dygraphs
dygraph(birthsSubset, main="Birth rates", ylab="Births", xlab="Year") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1955-1-1", to = "1956-1-1", color = "#FFE6E6") %>%
dyShading(from = "1959-1-1", to = "1960-1-1", color = "#CCEBD6")
dygraph(Wand, main="Wands Sold by Year", ylab="Units", xlab="Date", group = "Maker") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1995-1-1", to = "1956-1-1", color = "#FFE6E6") %>%
dyShading(from = "1999-1-1", to = "1960-1-1", color = "#CCEBD6")
Gregorovitch <- xts(Greg$Units, order.by = Greg$Date)
Ollivander <- xts(Ollie$Units, order.by = Ollie$Date)
Wand <- merge(Gregxts, Olliexts)
dygraph(Wand, main="Wands Sold by Year", ylab="Units", xlab="Date", group = "Maker") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1995-1-1", to = "1956-1-1", color = "#FFE6E6") %>%
dyShading(from = "1999-1-1", to = "1960-1-1", color = "#CCEBD6")
dygraph(Wand, main="Wands Sold by Year", ylab="Units", xlab="Date", group = "Maker") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1995-1-1", to = "1999-1-1", color = "#FFE6E6") %>%
dygraph(Wand, main="Wands Sold by Year", ylab="Units", xlab="Date", group = "Maker") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1995-1-1", to = "1999-1-1", color = "#FFE6E6")
dygraph(Wand, main="Wands Sold by Year", ylab="Units", xlab="Date", group = "Maker") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1995-1-1", to = "1999-1-1", color = "#FFE6E6")
dygraph(Wand, main="Wands Sold by Year", ylab="Units", xlab="Date", group = "Maker") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1995-1-1", to = "1999-1-1", color = "#FFE6E6")
library(dygraphs)
library(sqldf)
library(xts)
Greg <- read.csv("Unit11TimeSeries_Gregorovitch.csv", header = FALSE)
Ollie <- read.csv("Unit11TimeSeries_Ollivander.csv", header = FALSE)
class(Greg$V1)
Greg$V1 <- as.Date(Greg$V1, format = "%m/%d/%Y")
Ollie$V1 <- as.Date(Ollie$V1, format = "%m/%d/%Y")
Greg$Maker <- "Gregorovitch"
Ollie$Maker <- "Ollivander"
colnames(Greg)[colnames(Greg)=="V1"] <- "Date"
colnames(Greg)[colnames(Greg)=="V2"] <- "Units"
colnames(Ollie)[colnames(Ollie)=="V1"] <- "Date"
colnames(Ollie)[colnames(Ollie)=="V2"] <- "Units"
Gregorovitch <- xts(Greg$Units, order.by = Greg$Date)
Ollivander <- xts(Ollie$Units, order.by = Ollie$Date)
Wand <- merge(Gregxts, Olliexts)
Wand <- merge(Gregorovitch, Ollivander)
dygraph(Wand, main="Wands Sold by Year", ylab="Units", xlab="Date", group = "Maker") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1995-1-1", to = "1999-1-1", color = "#FFE6E6")
?dygraph
dygraph(Wand, main="Wands Sold by Year", ylab="Units", xlab="Date", group = "Maker", col = "Red") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1995-1-1", to = "1999-1-1", color = "#FFE6E6")
dygraph(Wand, main="Wands Sold by Year", ylab="Units", xlab="Date", group = "Maker") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1995-1-1", to = "1999-1-1", color = "#FFE6E6")
dygraph(Wand, main="Wands Sold by Year", ylab="Units", xlab="Date", group = "Maker") %>%
dyRangeSelector(height=100) %>%
dyShading(from = "1995-1-1", to = "1999-1-1", color = "#FFE6E6")
install.pcakges("forecast")
install.packages("forecast")
library(sqldf)
knitr::opts_chunk$set(echo = TRUE)
read.csv("C:/Users/Marin Family/Desktop/Kaggle/Titanic")
read.csv("C:/Users/Marin Family/Desktop/Kaggle/Titanic")
setwd("C:/Users/Marin Family/Desktop/Kaggle/Titanic")
train <- read.csv("train.csv")
View(train)
View(train)
?grep
grep
grep(Name, "Master")
grep("Master", train$Name)
Masters <- grep("Master", train$Name)
hist(train$Age)
hist(train$Sex)
hist(train$Fare)
ggplot(train, aes(x=Age, fill=cond)) +
geom_histogram(binwidth=.5, alpha=.5, position="identity")
library(ggplot2)
ggplot(train, aes(x=Age, fill=cond)) +
geom_histogram(binwidth=.5, alpha=.5, position="identity")
ggplot(train, aes(x=Age, fill=Survived)) +
geom_histogram(binwidth=.5, alpha=.5, position="identity")
ggplot(train, aes(x=Age)) +
geom_histogram(binwidth=.5, alpha=.5, position="identity")
ggplot(train, aes(x=Age)) +
geom_histogram(binwidth=5, alpha=.5, position="identity")
?ggplot
ggplot(train, aes(x=Age)) +
geom_histogram(binwidth=5, alpha=.5, position="identity") +
geom_point(train, aes(y=Surived))
ggplot(train, aes(x=Age)) +
geom_histogram(binwidth=5, alpha=.5, position="identity")
library(ggplot2)
library(sqldf)
Survived <- sqldf("select * from train where survived = '1'")
Survived <- sqldf("select * from train where survived = '1'")
Died <- sqldf("select * from train where survived = '0'")
ggplot(Died, aes(x=Age)) +
geom_histogram(binwidth=5, alpha=.5, position="identity")
ggplot(Survived, aes(x=Age)) +
geom_histogram(binwidth=5, alpha=.5, position="identity")
ggplot(Died, aes(x=Age)) +
geom_histogram(binwidth=5, alpha=.5, position="identity")
ggplot(Survived, aes(x=Age)) +
geom_histogram(binwidth=5, alpha=.5, position="identity")
ggplot(train, aes(x=Age)) +
geom_histogram(binwidth=5, alpha=.5, position="identity")
quantile(Surived, probs = seq(0,1, by = .1)
quantile(Surived, probs = seq(0,1, by = .1))
quantile(Survived, probs = seq(0,1, by = .1))
?quantile
quantile(Survived, probs = seq(0,1, by = .1), na.rm = TRUE)
quantile(Survived$Age, probs = seq(0,1, by = .1), na.rm = TRUE)
ggplot(train, aes(x=Sex, group = Survived, fill = Survived)) +
geom_histogram(stat = "count", aes(y=(..count..)/sum(..count..)),binwidth=5, alpha=.5, position="identity")+   facet_grid(~Survived, labeller = label_both) + theme(legend.position="none")
ggplot(train, aes(x=Sex, group = Survived, fill = Survived)) +
geom_histogram(stat = "count", aes(y=(..count..)/sum(..count..)),binwidth=5, alpha=.5, position="identity")+   facet_grid(~Survived, labeller = label_both) + theme(legend.position="none")
View(train)
geom_histogram(data = train, group = Pclass)
ggplot(train, aes(x=Sex, group = Survived, fill = Survived)) +
geom_histogram(stat = "count", aes(y=(..count..)/sum(..count..)),binwidth=5, alpha=.5, position="identity")+   facet_grid(~Survived, labeller = label_both) + theme(legend.position="none") +
geom_histogram(data = train, group = Pclass)
install.packages("easyGgplot2")
